---
title: "**Ski jump distance forecasting - GAM model**"
author: "Wiktor Florek"
date: '`r Sys.Date()`'
output:
  html_document:
    toc: yes
    number_sections: yes
    code_folding: hide
    theme: cosmo
    highlight: tango
---
```{r setup, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(include=TRUE, message=FALSE, warning=FALSE)
knitr::opts_chunk$set(out.width="100%", fig.height = 4.5, split=FALSE, fig.align = 'default')
options(dplyr.summarise.inform = FALSE)
```
# Introduction and objectives

One of the brightest aims while creating this ski jumping database was:

*Can I use this data to sensible future prediction of distances achieved by the athletes?*

I have a strong feeling that the answer is affirmative, because we have a lot of credible predictors like

* tangential wind speed
* inrun speed / start gate height
* ratings system, which measure the long-time potential of every jumper

et cetera. Of course these factors alone still leave a substantial amount of variance and that is an
information which is known to every ski jumping fan - 
*every minor or unobserved change can cause a huge difference in final distance*.
We can express this property simply as the quasi-chaotic behavior of our
target variable. Therefore we must be aware of the fact that the technique used has to handle these subtleties.

# Our basic approach - linear regression and Generalized Addivite Model (GAM)

First we will use an obvious base model - linear regression (without any penalties on the coefficients, because we have a huge amount of data to process and overfitting should not be present).

To make this introduction concise, [here](https://www.kaggle.com/wrotki8778/nyc-airbnb-price-modeling-by-gam) you have my markdown with an example of using GAM and its theoretical foundation. To our purpose the most important is to know that GAM is a hybrid of generalized linear models (like logistic or linear regression) and polynomial regression. GAM allows to capture nonlinear dependencies between predictors and the target variable, also (with some restrictions) using interactions between predictors.

# Data preparation

The necessary point is to clean and pre-process our dataset to be used in GAM:

```{r import}
all_ratings <- read.csv("../input/ski-jumping-results-database-2009now/all_ratings.csv")
all_comps_r <- read.csv("../input/ski-jumping-results-database-2009now/all_comps_r.csv")
all_comps_r['hill_type'] = cut(all_comps_r$hill_size_x,c(0,114,160,300),labels=c('normal','large','flying'))
all_results <- read.csv("../input/ski-jumping-results-database-2009now/all_results.csv")
all_names <- read.csv("../input/ski-jumping-results-database-2009now/all_names.csv")
all_names <- all_names[!duplicated(all_names[,c('codex')]),]
all_results <- all_results[all_results['speed']>50 & all_results['speed']<115,]
all_results <- all_results[all_results['dist']>40,]
dataset <- merge(all_results,all_comps_r,by=c('id'),all.y=FALSE)

dataset['norm_dist'] = dataset['dist']/dataset['hill_size_x']
dataset <- merge(dataset,all_ratings,by.x=c('id','codex.x','round'),by.y=c('id','codex','round'),all.y=FALSE,all.x=TRUE)
dataset$gender = as.integer(as.factor(dataset$gender))
train_dataset = dataset[(dataset['season']>2011) & (dataset['season']<2017),]
```

Dataset called *dataset* has a lot of variables, but we will use only columns called:

* *norm_dist* -- normalized distance of an athlete and our target variable (1 means that our jumper achieved hill size of an object),
* *speed* -- raw speed of an athlete in km/h,
* *wind* -- the mean tangential speed of wind (negative values correspond with tail wind, positive -- with head wind),
* *hill_type* -- factor with levels *normal*, *large* and *flying* indicating the size of an underlying object,
* *cumm_rating* -- rating of an athlete (1000 is the average, the bigger the better)
* *gender* -- 0/1 variable indicating men or ladies competition,
* *training* -- 0/1 variable indicating training rounds.

One specific column is worth to mention, because it appears the first time in our dataset:

* *short_rating* -- variable computed by the following method: take all previous jumps (training or competition) in the underlying weekend, calculate their deltas (i.e. see if the results of these are better or worse than the general level suggest) and take the mean. 

# Results - linear regression

Let's see how the basic linear regression model behaves:
(disclaimer: the *hill_type* interactions were added after the observation that different types of hills should have different slope coefficient by our predictors)

```{r linear regression}
simple_model <- lm(norm_dist~(speed+wind+short_rating+cumm_rating+gender+training)*hill_type+hill_type, data=train_dataset)
summary(simple_model)
```

As we can see, all variables are significant for this model. If we assume that the $\lvert t \rvert$ statistic is the ad-hoc chosen measure of importance, then we infer that the first five components of our model are the most important in terms of prediction power. The $R^2$ coefficient says what amount of the variance can be explained by this model. We can improve that benchmark by GAM quite easily. The residuals of our model also are quite small, though not negligible (compare with the initial distribution of *norm_dist* below).

```{r}
hist(train_dataset$norm_dist)
```

We can ask *OK, but what do these results mean when we try to make the real predictions?* To answer that question we will check the predictions of our model in the following 3 cases:

1. World Cup in Trondheim in 2017 (code 2017JP3817RL), which was pretty standard competition without any major weather surprises and other problems.
2. World Ski Championships on Normal Hill in Seefeld in 2019 (code 2019JP3192RL), which was exactly the opposite of the first case, so it can be a good benchmark of robustness of our model,
3. Ski Flying World Cup in Kulm in 2018 (code 2018JP3068RL), where we will check the behavior on flying events.

Now we will create appropriate test set and see how well our linear model generalizes:

```{r test}
require(tidyverse)
require(knitr)
codes = c('2017JP3817RL','2019JP3192RL','2018JP3068RL')
predict_comp <- function(code, dataset, model){
  test_dataset = dataset %>% filter(grepl(code, id)) %>% arrange(desc(cumm_rating),round)
  test_dataset = merge(test_dataset, all_names,how='left',by.x='codex.x',by.y='codex')
  test_dataset['output'] = predict(model,test_dataset)*test_dataset['hill_size_x']
  test_dataset['error'] = test_dataset['dist'] - test_dataset['output']
  names = c('name','round','c_rating','s_rating','dist','output','error')
  output_df = data.frame(test_dataset[,c('name','round','cumm_rating','short_rating','dist','output','error')])
  colnames(output_df)=names
  return(output_df)
}
for(code in codes) print(format.data.frame(predict_comp(code,dataset,simple_model),digits=1))
```

The basic thoughts about the results can be as below:

* In standard situations (like the first one, with little wind perturbations) this model behaves quite well, the errors are not obviously biased and the variance is not so huge.
* In some cases (like the second one) with the abnormal conditions the predictions are significantly biased, so the "better" model should erase this problem.
* In the case of flying hills we can observe quite high variance, although the general rule is OK, i.e. we have no obvious biases, what was the case without the interaction terms (then the model has been too conservative).

# Results - GAM

Now we will try the more advanced model, when the non-linearity is present:

```{r gam}
library(mgcv) # contains our prime model
model<-gam(norm_dist~s(speed, by = hill_type)+wind+wind:hill_type+s(cumm_rating, by = hill_type)+gender + gender:hill_type+training + training:hill_type+s(short_rating, by = hill_type), data=train_dataset)
summary(model)
```

The results suggest that the further development of the model increased the explained variance. Now we will check how it projects on the predictions phase:


```{r gam_test}
require(tidyverse)
require(knitr)
require(ggplot2)
predict_comp_two <- function(code, dataset, simple_model, model){
  test_dataset = dataset %>% filter(grepl(code, id)) %>% arrange(desc(cumm_rating),round)
  test_dataset = merge(test_dataset, all_names,how='left',by.x='codex.x',by.y='codex')
  test_dataset['l_output'] = predict(simple_model,test_dataset)*test_dataset['hill_size_x']
  test_dataset['output'] = predict(model,test_dataset)*test_dataset['hill_size_x']
  test_dataset['l_error'] = test_dataset['dist'] - test_dataset['l_output']
  test_dataset['error'] = test_dataset['dist'] - test_dataset['output']
  r_squared_gam = sum((test_dataset[,'error'])^2)/sum((test_dataset[,'dist']-mean(test_dataset[,'dist']))^2)
  r_squared_linear = sum((test_dataset[,'l_error'])^2)/sum((test_dataset[,'dist']-mean(test_dataset[,'dist']))^2)
  print(paste(1-r_squared_gam, 1-r_squared_linear))
  N = paste('Results from : ', code, ' with linear (red) and GAM model (green)')
  print(ggplot(test_dataset) + geom_jitter(aes(x = dist, y = output), color='green') + geom_jitter(aes(x = dist, y = l_output), color='red') + geom_abline(aes(intercept=0,slope=1), color='blue') + labs(title=N))
  output = test_dataset[,c('name','round','dist','l_output','l_error','output','error')]
  return(output)
}
for(code in codes) print(format.data.frame(predict_comp_two(code,dataset,simple_model,model),digits=1))
```

We can see that the biggest difference can be seen on the flying hills, but the problems remain similar.

# How we can try to improve the quality of our model?

I have been thought about these results and the basic explanation of the problems with our model can be as below:

The target variable is not only *jumper-dependent*, but also (if not mainly) *time-dependent*, i.e. the actual model errors should be positively correlated if the moment of jumps are similar.

If we introduce the time-dependency to our model, then most of our problems should be removed. How can we do that?

# Autoregressive (AR) prediction model

Before the code, I want to describe the basic equation of our model:

$\hat{L}_t = \sum\limits_{i=t-10}^{t-1} \alpha_i \epsilon_i + GAM_t$

where $\hat{L}_t$ is the predicted normalized length of the $t$-th jumper, $\epsilon_i = L_i - GAM_i$ is the *actual* error caused by the GAM approximation until time $t$ and $GAM_t$ is the GAM prediction of $L_t$.

The $\alpha$'s have to be estimated, but for now we will assume that $\alpha_{t-1} = 2/9$ and $\alpha_{t-(i+1)} = \alpha_{t-i} - 2/81$:

```{r ar_model}
shift <- function(x, n){
  if (length(x)<=n){return(rep(0,length(x)))}
  return(c(rep(x[1], n),x[seq(length(x)-n)]))
}
ar_predict <- function(dataset,model,code){
test_dataset = dataset %>% filter(grepl(code, id)) %>% arrange(round,bib)
hill_size = test_dataset['hill_size_x'][1]
test_dataset['norm_dist'] = test_dataset['dist']/hill_size
test_dataset['gam_t'] = predict(model,test_dataset)
test_dataset['output'] = test_dataset['gam_t']
test_dataset['epsilon_t'] = test_dataset['norm_dist'] - test_dataset['gam_t']
coeffs = seq(2/9,0,length.out=9)
for(i in 1:9){
  name = paste('epsilon_t',-i, sep='')
  test_dataset[name] = shift(test_dataset$epsilon_t,i)
  test_dataset['output'] = test_dataset['output']+coeffs[i]*test_dataset[name]
}
test_dataset['error'] = test_dataset['norm_dist'] - test_dataset['output']
r_squared_pr = sum((test_dataset[,'error'])^2)/sum((test_dataset[,'norm_dist']-mean(test_dataset[,'norm_dist']))^2)
r_squared_gam = sum((test_dataset[,'epsilon_t'])^2)/sum((test_dataset[,'norm_dist']-mean(test_dataset[,'norm_dist']))^2)
print(paste(1-r_squared_pr, 1-r_squared_gam))
N = paste('Results from : ', code, ' with GAM (red) and AR model (green)')
print(ggplot(test_dataset) + geom_jitter(aes(x = norm_dist, y = output), color='green') + geom_jitter(aes(x = norm_dist, y = gam_t), color='red') + geom_abline(aes(intercept=0,slope=1), color='blue') + labs(title=N) + xlim(0.6,1.1) + ylim(0.6,1.1))
test_dataset = test_dataset %>% 
  mutate(gam_t = round(gam_t * hill_size_x,1),
          epsilon_t = round(epsilon_t * hill_size_x,1),
          output = round(output * hill_size_x,1),
          error = round(error * hill_size_x,1))
test_dataset = merge(test_dataset, all_names,how='left',by.x='codex.x',by.y='codex')
output = test_dataset[,c('name','round','dist','gam_t','epsilon_t','output','error')]
return(output)
}
for(code in codes) print(ar_predict(dataset,simple_model,code))
```

The fast look on the data and we can see that the outputs are substantially different, if the model is biased or the conditions changes fastly. Although sometimes the residuals are bigger under the time-dependent model, the ad-hoc $R^2$ measures printed in the console say that the variance explained is significantly higher, so the model can be quite safely considered as the better one, mainly because the biases are removed.

(alert: these $R^2$ statistics do not seem to be exciting (here between 0.3 and 0.6 roughly), but we are referencing to the mean of distances in a given competition, which can differ significantly from the overall mean. Therefore the negative value of $R^2$ in the GAM second case.)

Of course that is not the end of the story, because the $\alpha$ coefficients were taken a priori. We can now estimate their optimal values in the same fashion as in the standard linear regression:

```{r estimation_of_alphas}
ar_estimate <- function(dataset,model,code){
test_dataset = dataset %>% filter(grepl(code, id)) %>% arrange(round,bib)
hill_size = test_dataset['hill_size_x'][1]
test_dataset['norm_dist'] = test_dataset['dist']/hill_size
test_dataset['gam_t'] = predict(model,test_dataset)
test_dataset['epsilon_t'] = test_dataset['norm_dist'] - test_dataset['gam_t']
coeffs = seq(2/9,0,length.out=9)
names=c()
for(i in 1:9){
  name = paste('epsilon_t',-i, sep='')
  names = c(names,name)
  test_dataset[name] = shift(test_dataset$epsilon_t,i)
}
output = test_dataset[,c(c('epsilon_t'),names)]
return(output)
}
all_codes = c(all_comps_r[(all_comps_r['training']==0) & (all_comps_r['season']>2014) & (all_comps_r['team'] == 0) & (1-is.na(all_comps_r['wind.factor'])),'id'])
set.seed(2)
sample_codes = sample(all_codes,150)
test_dataset = ar_estimate(dataset,model,sample_codes[1])
for(i in 2:length(sample_codes)){
  test_dataset = rbind(test_dataset,ar_estimate(dataset,model,sample_codes[i]))
}
estimates_model = lm(epsilon_t ~ ., data=test_dataset)
summary(estimates_model)
```

We conclude that the time window is properly adjusted (i.e. most of these $\epsilon$'s are significant at least with the confidence of 95%). The weights also are roughly correct, because the most recent observations receive the highest weights.

# Conclusions

We have implemented the time and static approach into our prediction model. It could be seen that both of these methods are necessary to make this model useful and flexible. 

Of course the presented algorithm has its limitations. Because we have few predictors, even if their quality is good, the model will show its underfitting tendencies. To avoid this problem, it's possible to use deep recurrent methods like LSTM, but it will be considered later. The other alternative is to add new predictors possibly uncorrelated with the previous and of good predictive power. Basic candidates could be somewhat like:

* the mid-term rating (i.e. mean rating movement over 3 weeks or a similar amount of time),
* the flying hill performance (i.e. mean rating movement on the largest hills, the 3rd test case proves that some athletes can perform better than the other rating variables suggest)
* more information about wind (i.e. the mean of tangential wind speed is sometimes misleading and the raw values in single sensors probably could be much more helpful)

In the other words, more data would probably help to build a better model.
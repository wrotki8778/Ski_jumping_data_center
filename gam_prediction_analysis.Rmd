---
title: "Ski jump distance forecasting - GAM model"
author: "Wiktor Florek"
date: "12 06 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction and objectives

One of the brightest aims while creating this ski jumping database was:

*Can I use this data to sensible future prediction of distances achieved by the athletes?*

I have a strong feeling that the answer is affirmative, because we have a lot of credible predictors like

* tangential wind speed
* inrun speed / start gate height
* ratings system, which measure the long-time potential of every jumper

et cetera. Of course these factors alone still leave a substantial amount of variance and that is an
information which is known to every ski jumping fan - 
*every minor or unobserved change can cause a huge difference in final distance*.
We can express this property simply as the quasi-chaotic behavior of our
target variable. Therefore we must be aware of the fact that the technique used has to handle these subtleties.

# Our basic approach - linear regression and Generalized Addivite Model (GAM)

First we will use an obvious base model - linear regression (without any penalties on the coefficients, because we have a huge amount of data to process and overfitting should not be present).

To make this introduction concise, [here](https://www.kaggle.com/wrotki8778/nyc-airbnb-price-modeling-by-gam) you have my markdown with an example of using GAM and its theoretical foundation. To our purpose the most important is to know that GAM is a hybrid of generalized linear models (like logistic or linear regression) and polynomial regression. GAM allows to capture nonlinear dependencies between predictors and the target variable, also (with some restrictions) using interactions between predictors.

# Data preparation

The necessary point is to clean and pre-process our dataset to be used in GAM:

```{r import}
all_ratings <- read.csv("all_ratings.csv")
all_comps_r <- read.csv("all_comps_r.csv")
all_results <- read.csv("all_results.csv")
all_names <- read.csv("all_names.csv")
all_names <- all_names[!duplicated(all_names[,c('codex')]),]
all_results <- all_results[all_results['speed']>50 & all_results['speed']<115,]
all_results <- all_results[all_results['dist']>40,]
dataset <- merge(all_results,all_comps_r,by=c('id'),all.y=FALSE)

dataset['norm_dist'] = dataset['dist']/dataset['hill_size_x']
dataset <- merge(dataset,all_ratings,by.x=c('id','codex.x','round'),by.y=c('id','codex','round'),all.y=FALSE,all.x=TRUE)
dataset$gender = as.integer(as.factor(dataset$gender))
dataset$date_new = as.integer(as.Date.character(dataset$date))
train_dataset = dataset[(dataset['season']>2011) & (dataset['season']<2017),]
```

Dataset called *dataset* has a lot of variables, but we will use only columns called:

* *norm_dist* -- normalized distance of an athlete and our target variable (1 means that our jumper achieved hill size of an object),
* *speed* -- raw speed of an athlete in km/h,
* *wind* -- the mean tangential speed of wind (negative values correspond with tail wind, positive -- with head wind),
* *hill_size_x* -- the hill size of an underlying object, where the competition is held,
* *cumm_rating* -- rating of an athlete (1000 is the average, the bigger the better)
* *gender* -- 0/1 variable indicating men or ladies competition,
* *date_new* -- integer indicating day of the competition (reformatted from the standard time scale to be able to included in GAM)
* *training* -- 0/1 variable indicating training rounds.

One specific column is worth to mention, because it appears the first time in our dataset:

* *short_rating* -- variable computed by the following method: take all previous jumps (training or competition) in the underlying weekend, calculate their deltas (i.e. see if the results of these are better or worse than the general level suggest) and take the mean. 

# Results - linear regression

Let's see how the basic linear regression model behaves:

```{r linear regression}
simple_model <- lm(norm_dist~speed+wind+hill_size_x+cumm_rating+gender+date_new+training+short_rating, data=train_dataset)
summary(simple_model)
```

As we can see, all variables are significant for this model. If we assume that the $\lvert t \rvert$ statistic is the ad-hoc chosen measure of importance, then we infer that the first five components of our model are the most important in terms of prediction power. The $R^2$ coefficient says that almost 50% of the variance can be explained by this model, what is pretty good result. Nevertheless we can improve that benchmark by GAM quite easily. The residuals of our model also are quite small, though not negligible (compare with the initial distribution of *norm_dist* below).

```{r}
hist(train_dataset$norm_dist)
```

We can ask *OK, but what do these results mean when we try to make the real predictions?* To answer that question we will check the predictions of our model in the following 3 cases:

1. World Cup in Trondheim in 2017 (code 2017JP3817RL), which was pretty standard competition without any major weather surprises and other problems.
2. World Ski Championships on Normal Hill in Seefeld in 2019 (code 2019JP3192RL), which was exactly the opposite of the first case, so it can be a good benchmark of robustness of our model,
3. Ski Flying World Cup in Kulm in 2018 (code 2018JP3068RL), where we will check the behavior on flying events.

Now we will create appropriate test set and see how well our linear model generalizes:

```{r test}
require(tidyverse)
require(knitr)
codes = c('2017JP3817RL','2019JP3192RL','2018JP3068RL')
predict_comp <- function(code, dataset, model){
  test_dataset = dataset %>% filter(grepl(code, id)) %>% arrange(desc(cumm_rating),round)
  test_dataset = merge(test_dataset, all_names,how='left',by.x='codex.x',by.y='codex')
  test_dataset['prediction'] = predict(model,test_dataset)*test_dataset['hill_size_x']
  test_dataset['prediction'] = round(2*test_dataset['prediction'])/2
  test_dataset['error'] = test_dataset['dist'] - test_dataset['prediction']
  kable(test_dataset[,c('name','round','cumm_rating','short_rating','dist','prediction','error')])
}
for(code in codes) print(predict_comp(code,dataset,simple_model))
```

The basic thoughts about the results can be as below:

* In standard situations (like the first one, with little wind perturbations) this model behaves quite well, the errors are not obviously biased and the variance is not so huge.
* In some cases (like the second one) with the abnormal conditions the predictions are significantly biased, so the "better" model should erase this problem.
* In the case of flying hills we can see the conservative nature of this model -- the predicted values range between 170 and 215 meters, so in this setting we also should search an alternative approach.

# Results - GAM

Now we will try the more advanced model, when the non-linearity is present:

```{r gam}
library(mgcv) # contains our prime model
model<-gam(norm_dist~s(speed)+wind+hill_size_x+s(cumm_rating)+gender+training+s(date_new)+s(short_rating), data=train_dataset)
summary(model)
```

The results suggest that the further development of the model increased the explained variance from 48% to almost 51%. Now we will check how it projects on the predictions phase:


```{r gam_test}
require(tidyverse)
require(knitr)
require(ggplot2)
predict_comp_two <- function(code, dataset, simple_model, model){
  test_dataset = dataset %>% filter(grepl(code, id)) %>% arrange(desc(cumm_rating),round)
  test_dataset = merge(test_dataset, all_names,how='left',by.x='codex.x',by.y='codex')
  test_dataset['linear_prediction'] = predict(simple_model,test_dataset)*test_dataset['hill_size_x']
  test_dataset['linear_prediction'] = round(2*test_dataset['linear_prediction'])/2
  test_dataset['prediction'] = predict(model,test_dataset)*test_dataset['hill_size_x']
  test_dataset['prediction'] = round(2*test_dataset['prediction'])/2
  test_dataset['linear_error'] = test_dataset['dist'] - test_dataset['linear_prediction']
  test_dataset['error'] = test_dataset['dist'] - test_dataset['prediction']
  r_squared_gam = sum((test_dataset[,'error'])^2)/sum((test_dataset[,'dist']-mean(test_dataset[,'dist']))^2)
  r_squared_linear = sum((test_dataset[,'linear_error'])^2)/sum((test_dataset[,'dist']-mean(test_dataset[,'dist']))^2)
  print(paste(1-r_squared_gam, 1-r_squared_linear))
  N = paste('Results from : ', code, ' with linear (red) and GAM model (green)')
  print(ggplot(test_dataset) + geom_jitter(aes(x = dist, y = prediction), color='green') + geom_jitter(aes(x = dist, y = linear_prediction), color='red') + geom_abline(aes(intercept=0,slope=1), color='blue') + labs(title=N))
  output = test_dataset[,c('name','round','dist','linear_prediction','linear_error','prediction','error')]
  return(output)
}
for(code in codes) print(predict_comp_two(code,dataset,simple_model,model))
```

We can see that the biggest difference can be seen on the flying hills, but the problems remain similar.

# How we can try to improve the quality of our model?

I have been thought about these results and the basic explanation of the problems with our model can be as below:

The target variable is not only *jumper-dependent*, but also (if not mainly) *time-dependent*, i.e. the actual model errors should be positively correlated if the moment of jumps are similar.

If we introduce the time-dependency to our model, then most of our problems should be removed. How can we do that?

# Autoregressive (AR) prediction model

Before the code, I want to describe the basic equation of our model:

$\hat{L}_t = \sum\limits_{i=t-10}^{t-1} \alpha_i \epsilon_i + GAM_t$

where $\hat{L}_t$ is the predicted normalized length of the $t$-th jumper, $\epsilon_i = L_i - GAM_i$ is the *actual* error caused by the GAM approximation until time $t$ and $GAM_t$ is the GAM prediction of $L_t$.

The $\alpha$'s have to be estimated, but for now we will assume that $\alpha_{t-1} = 2/9$ and $\alpha{t-(i+1)} = \alpha{t-i} - 2/81$:

```{r ar_model}
shift <- function(x, n){
  return(c(rep(x[1], n),x[seq(length(x)-n)]))
}
ar_predict <- function(dataset,model,code){
test_dataset = dataset %>% filter(grepl(code, id)) %>% arrange(round,bib)
hill_size = test_dataset['hill_size_x'][1]
test_dataset['norm_dist'] = test_dataset['dist']/hill_size
test_dataset['gam_t'] = predict(model,test_dataset)
test_dataset['prediction'] = test_dataset['gam_t']
test_dataset['epsilon_t'] = test_dataset['norm_dist'] - test_dataset['gam_t']
coeffs = seq(2/9,0,length.out=9)
for(i in 1:9){
  name = paste('epsilon_t',-i, sep='')
  test_dataset[name] = shift(test_dataset$epsilon_t,i)
  test_dataset['prediction'] = test_dataset['prediction']+coeffs[i]*test_dataset[name]
}
test_dataset['error'] = test_dataset['norm_dist'] - test_dataset['prediction']
r_squared_pr = sum((test_dataset[,'error'])^2)/sum((test_dataset[,'norm_dist']-mean(test_dataset[,'norm_dist']))^2)
r_squared_gam = sum((test_dataset[,'epsilon_t'])^2)/sum((test_dataset[,'norm_dist']-mean(test_dataset[,'norm_dist']))^2)
print(paste(1-r_squared_pr, 1-r_squared_gam))
N = paste('Results from : ', code, ' with GAM (red) and AR model (green)')
print(ggplot(test_dataset) + geom_jitter(aes(x = norm_dist, y = prediction), color='green') + geom_jitter(aes(x = norm_dist, y = gam_t), color='red') + geom_abline(aes(intercept=0,slope=1), color='blue') + labs(title=N) + xlim(0.6,1.1) + ylim(0.6,1.1))
test_dataset = test_dataset %>% 
  mutate(gam_t = round(gam_t * hill_size_x,1),
          epsilon_t = round(epsilon_t * hill_size_x,1),
          prediction = round(prediction * hill_size_x,1),
          error = round(error * hill_size_x,1))
test_dataset = merge(test_dataset, all_names,how='left',by.x='codex.x',by.y='codex')
output = test_dataset[,c('name','round','dist','gam_t','epsilon_t','prediction','error')]
return(output)
}
for(code in codes) print(ar_predict(dataset,simple_model,code))
```

The fast look on the data and we can see that the outputs are substantially different, if the model is biased or the conditions changes fastly. Although sometimes the residuals are bigger under the time-dependent model, the ad-hoc $R^2$ measures printed in the console say that the variance explained is significantly higher, so the model can be quite safely considered as the better one, mainly because the biases are removed.

(alert: these $R^2$ statistics do not seem to be exciting (here between 0.5 and 0.6 roughly), but we are referencing to the mean of distances in a given competition, which can differ significantly from the overall mean. Therefore the negative value of $R^2$ in the GAM second case.)

Of course that is not the end of the story, because the $\alpha$ coefficients were taken a priori. We can now estimate their optimal values in the same fashion as in the standard linear regression:

```{r estimation_of_alphas}
ar_estimate <- function(dataset,model,code){
test_dataset = dataset %>% filter(grepl(code, id)) %>% arrange(round,bib)
hill_size = test_dataset['hill_size_x'][1]
test_dataset['norm_dist'] = test_dataset['dist']/hill_size
test_dataset['gam_t'] = predict(model,test_dataset)
test_dataset['epsilon_t'] = test_dataset['norm_dist'] - test_dataset['gam_t']
coeffs = seq(2/9,0,length.out=9)
names=c()
for(i in 1:9){
  name = paste('epsilon_t',-i, sep='')
  names = c(names,name)
  test_dataset[name] = shift(test_dataset$epsilon_t,i)
}
output = test_dataset[,c(c('epsilon_t'),names)]
return(output)
}
all_codes = c(all_comps_r[(all_comps_r['training']==0) & (all_comps_r['season']>2014) & (all_comps_r['team'] == 0) & (1-is.na(all_comps_r['wind.factor'])),'id'])
set.seed(2)
sample_codes = sample(all_codes,50)
test_dataset = ar_estimate(dataset,model,sample_codes[1])
for(i in 2:length(sample_codes)){
  test_dataset = rbind(test_dataset,ar_estimate(dataset,model,sample_codes[i]))
}
estimates_model = lm(epsilon_t ~ ., data=test_dataset)
summary(estimates_model)
```

We conclude that the time window is properly adjusted (i.e. all of these $\epsilon$'s are significant at least with the confidence of 95%). The weights also are roughly correct, because the most recent observations receive the highest weights.
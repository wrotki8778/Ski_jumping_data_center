---
title: "Initial SJ markdown"
author: "WF"
date: "07 02 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Introduction

The analysis below corresponds with the data uploaded yesterday (27.01.21) on kaggle.com. In this case we will not go beyond the basic EDA. I will mainly concentrate on the file containing information about competitions. 

# How these data look like?

Lets begin with the data and libraries import:

```{r import}
library(tidyverse)
library(e1071)
competitions = read.csv('all_comps_r.csv',sep=',',dec='.',stringsAsFactors = TRUE)
results = read.csv('all_results.csv',sep=',',dec='.')
names = read.csv('all_names.csv',sep=',',dec='.',stringsAsFactors = TRUE)

summary(competitions)
summary(results)
```
In fact in competitions dataset we have mostly integer variables. What is interesting only from this code is that

* the NA values are apparent - mostly in these cases, when 
  + the wind/gate factor was not taken into account (roughly 25% of competitions and 20% of jumps, mainly in COC/FC competitions)
  + the style was not counted (all training rounds, roughly 45% of total number of jumps)
  + parsing of PDF file was not successful (roughly 6% of competitions, mainly before 2012/13 and for COC/FC types)
* most competitions was held in Germany and in Norway.

# Some insight plots - competitions dataset

Lets see the most fashioned places and countries, grouped also by the type of competition
```{r barplots, echo=FALSE}
counts <- as.data.frame(table(competitions$place,competitions$gender))
counts_raw <- counts[aggregate(Freq ~ Var1,counts, sum)$Freq>50,]
counts <- filter(counts,counts$Var1 %in% counts_raw$Var1)
counts %>%
  mutate(Var1 = fct_reorder(Var1,Freq,sum)) %>%
  ggplot(aes(x=Var1, y=Freq, fill=Var2)) +
    geom_bar(stat="identity", alpha=.6, width=.4) +
    coord_flip() +
    scale_fill_discrete(name = "Gender") +
    xlab("") +
    theme_bw()

countries <- as.data.frame(table(competitions$country,competitions$gender))
countries_raw <- countries[aggregate(Freq ~ Var1,countries, sum)$Freq>50,]
countries <- filter(countries,countries$Var1 %in% countries_raw$Var1)
countries %>%
  mutate(Var1 = fct_reorder(Var1,Freq,sum)) %>%
  ggplot(aes(x=Var1, y=Freq, fill=Var2)) +
    geom_bar(stat="identity", alpha=.6, width=.4) +
    coord_flip() +
    scale_fill_discrete(name = "Gender") +
    xlab("") +
    theme_bw()

competitions['type_str'] = cut(competitions$type, breaks = c(-0.5,0.5,1.5,2.5,3.5,5.5), labels=c('WC', 'COC', 'GP','FC','Other')) 

types <- as.data.frame(table(competitions$place,competitions$type_str))
types_raw <- types[aggregate(Freq ~ Var1,types, sum)$Freq>100,]
types <- filter(types,types$Var1 %in% types_raw$Var1)
types %>%
  mutate(Var1 = fct_reorder(Var1,Freq,sum)) %>%
  ggplot(aes(x=Var1, y=Freq, fill=Var2)) +
    geom_bar(stat="identity", alpha=.6, width=.4) +
    scale_fill_discrete(name = "Type") +
    coord_flip() +
    xlab("") +
    theme_bw()
```

As we can see, various hills and cities have different "target" and its own specificity in terms of organizing ski jumping competitions. Let's see the sizes of different objects appearing in the dataset and the counts:

```{r size}
competitions['size'] = cut(competitions$hill_size_x, breaks = c(0,85,120,160,300), labels=c('small', 'normal', 'large','flying hill')) 
ggplot(subset(competitions, gender %in% c("Men", "Women"))) +
  geom_bar(aes(x = size, color=gender),fill="white", alpha = 0.6, position="dodge")
ggplot(competitions) +
  geom_bar(aes(x = size, color=type_str),fill="white", alpha = 0.6, position="dodge")
```

From these figures we can see that

* women compete mostly on the normal hills and men jump mainly on the large objects,
* women participate in substantially less number of competitions than men,
* most NAs come from COC competitions (season 2010/11 with different format of PDF files),
* the leagues with lower rank (COC, GP, FC) tend to organize competitions on smaller hills, what doesn't surprise.

# Some insight plots - results dataset

Now we will try to say something about the "main" dataset, which comprises the results of the competitions mentioned above. First thing is that (unfortunately) some competitions from the competitions dataset don't appear in the main dataset, exactly we have the number of not-processed PDFs:

```{r missing}
length(setdiff(competitions$id,results$id))
```
That is about 10% of "missing" comps, and this is the result of problems while parsing (mainly from the years before 2012). I still try to include this files into the dataset, but that won't be easy.
Now we can plot the basic facts about the key columns. First we plot the density of speed variable:
```{r speed}
ggplot(subset(results, speed > 60 & speed < 120)) +
  geom_density(aes(x = speed),fill="white", alpha = 0.6)
ggplot(subset(merge(results[,c('speed','id')],competitions,by='id'), speed > 60 & speed < 120)) +
  geom_area(aes(x = speed, color=size),fill="white", alpha = 0.6, stat='bin', position = position_dodge())
```

We notice that the in-run speed varies mostly between 80 and 95 kilometers per hour, but this (as we can see further) is determined by the hill size (HS) of a given object. We can derive a similar plot in terms of achieved distance.

```{r distances}
ggplot(subset(results, dist>0)) +
  geom_density(aes(x = dist),fill="white", alpha = 0.6)
ggplot(subset(merge(results[,c('dist','id')],competitions[,c('id','size')],by='id'), dist>0)) +
  geom_area(aes(x = dist, color=size),fill="white", alpha = 0.6, stat='bin', position = position_dodge())
ggplot(subset(merge(results[,c('dist','id')],competitions[,c('id','size')],by='id'), dist>0)) +
  geom_density(aes(x = dist, color=size),fill="white", alpha = 0.6)+labs(title='Normalized plots of conditional densities')
```

What can be unexpected is that the variance of achieved distance is slightly higher on normal hills than on the large ones. In this conditional case the distribution is quite similar to normal, but it should be noted that the skewness is present here and is negative on every type of hill:

```{r skewness}
large_hill_data = subset(merge(results[,c('dist','id')],competitions[,c('id','size')],by='id'), dist>0 & size=='large')
summary(large_hill_data$dist)
skewness(large_hill_data$dist)
```

Next, we take a look on the additional variables like note_points and wind:

```{r style_1}
ggplot(results) +
  geom_density(aes(x = note_points),fill="white", alpha = 0.6)
```

We can see that this plot is weirdly ragged. This can be explained by the fact, that the judges tend to give the same score, so the values 51 (3 times 17.0), 52.5 (3 times 17.5) etc. are preferred. The conclusion is that it's more acceptable to take only one judge and see the distribution.

```{r style_2}
ggplot(results) +
  geom_histogram(aes(x = note_1, y=..density..), breaks=seq(0,20,by=0.5))
```

As an exercise we will estimate the distribution by the beta family of distributions. Next we visualize how well our distribution is approximated by this approach.

```{r style_beta}
library(EnvStats)

parameters=ebeta(results$note_1/20)
print(parameters)

ggplot(results) +
  geom_histogram(aes(x = note_1/20, y=..density..), breaks=seq(0,1,length.out = 41)) +
  stat_function(fun = dbeta, args = list(shape1 = parameters$parameters[1], shape2 = parameters$parameters[2]))
```

Although the goodness of fit is not perfect, the nature of this variable is quite evident. Next we will see what is going on with the wind variable:

```{r wind}
summary(results$wind)
ggplot(results) +
  geom_density(aes(x = wind),fill="white", alpha = 0.6)
```

This plot is somewhat surprising, because it should be intuitive that the distribution of winds should be symmetric, but it does not meet that property. However, if we draw a density of *wind_comp* variable, then it becomes symmetric.

```{r wind_points}
summary(results$wind_comp)
ggplot(results) +
  geom_density(aes(x = wind_comp),fill="white", alpha = 0.6)
```

The purpose of this result is that the wind compensations are adjusted -- the bonus from the tail wind is bigger than the penalty from the head wind. As we can see, this method works quite well.

## Conclusions

As we could see, even these aggregate way of using our data brings quite attractive plots. Of course, by taking both of these datasets together, we can do a lot more of statistical analysis. I am sure that these observations contain a massive amount of knowledge which is not discovered yet.

If time allows, I will upload some more advanced markdowns with more particular insights. I have a few questions which should be answered within a few weeks or months. Maybe someone else also has an idea how use this information. See you later!

